Real-Time Credit Card Fraud Detection System - Development Prompt
Project Overview
Build a comprehensive real-time credit card fraud detection system using the provided dataset (1,157 transactions with 31 features including PCA-transformed variables V1-V28, Time, Amount, and Class labels where 0=normal, 1=fraud). The system should handle the extreme class imbalance (99.83% normal, 0.17% fraud) and provide real-time predictions with a modern web interface.
Dataset Analysis Results
Shape: 1,157 transactions Ã— 31 features


Features: Time, V1-V28 (PCA-transformed), Amount, Class


Class Distribution: 1,155 normal (99.83%), 2 fraud (0.17%)


Data Quality: No missing values, clean numerical data


Key Insights: Highly imbalanced dataset requiring specialized handling


System Architecture Requirements
PostgreSQL Database Design
sql
-- Main transactions table
CREATE TABLE transactions (
    id SERIAL PRIMARY KEY,
    transaction_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    time_feature INTEGER,
    v1 FLOAT, v2 FLOAT, v3 FLOAT, v4 FLOAT, v5 FLOAT,
    v6 FLOAT, v7 FLOAT, v8 FLOAT, v9 FLOAT, v10 FLOAT,
    v11 FLOAT, v12 FLOAT, v13 FLOAT, v14 FLOAT, v15 FLOAT,
    v16 FLOAT, v17 FLOAT, v18 FLOAT, v19 FLOAT, v20 FLOAT,
    v21 FLOAT, v22 FLOAT, v23 FLOAT, v24 FLOAT, v25 FLOAT,
    v26 FLOAT, v27 FLOAT, v28 FLOAT,
    amount DECIMAL(10,2),
    actual_class INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Predictions table
CREATE TABLE predictions (
    id SERIAL PRIMARY KEY,
    transaction_id INTEGER REFERENCES transactions(id),
    isolation_forest_score FLOAT,
    lstm_prediction FLOAT,
    ensemble_prediction FLOAT,
    final_prediction INTEGER,
    confidence_score FLOAT,
    model_version VARCHAR(50),
    prediction_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Model performance tracking
CREATE TABLE model_performance (
    id SERIAL PRIMARY KEY,
    model_name VARCHAR(100),
    precision_score FLOAT,
    recall_score FLOAT,
    f1_score FLOAT,
    auc_score FLOAT,
    evaluation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Alert logs
CREATE TABLE fraud_alerts (
    id SERIAL PRIMARY KEY,
    transaction_id INTEGER REFERENCES transactions(id),
    alert_level VARCHAR(20), -- HIGH, MEDIUM, LOW
    alert_reason TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    acknowledged BOOLEAN DEFAULT FALSE
);

Redis Caching Strategy
python
# Redis will be used for:
# 1. Model caching - Store trained models for quick access
redis_keys = {
    "model:isolation_forest": "Serialized Isolation Forest model",
    "model:lstm": "Serialized LSTM model weights",
    "model:ensemble": "Ensemble model configuration",
    "features:scaler": "Feature scaling parameters",
    "stats:daily": "Daily transaction statistics",
    "alerts:active": "Active fraud alerts list",
    "predictions:recent": "Last 1000 predictions cache",
    "performance:metrics": "Real-time performance metrics"
}

Apache Kafka Event Streaming
text
# Kafka Topics Configuration
topics:
  - name: "transactions-raw"
    description: "Raw transaction data from payment systems"
    partitions: 3
    replication_factor: 1
    
  - name: "transactions-processed"
    description: "Processed and feature-engineered transactions"
    partitions: 3
    replication_factor: 1
    
  - name: "fraud-predictions"
    description: "ML model predictions and confidence scores"
    partitions: 3
    replication_factor: 1
    
  - name: "fraud-alerts"
    description: "High-confidence fraud alerts for immediate action"
    partitions: 1
    replication_factor: 1

Backend Development (Python, Flask)
1. Data Processing Pipeline
python
# File: data_processor.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import redis
import psycopg2
from kafka import KafkaProducer, KafkaConsumer

class DataProcessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        
    def preprocess_transaction(self, transaction_data):
        """Handle extreme class imbalance with SMOTE/ADASYN"""
        # Implement preprocessing logic
        pass

2. Machine Learning Models
python
# File: ml_models.py
class FraudDetectionModels:
    def __init__(self):
        self.isolation_forest = None
        self.lstm_model = None
        self.ensemble_weights = {'isolation': 0.4, 'lstm': 0.6}
    
    def train_isolation_forest(self, X_train):
        """Unsupervised anomaly detection for fraud"""
        self.isolation_forest = IsolationForest(
            contamination=0.002,  # Based on your 0.17% fraud rate
            random_state=42,
            n_estimators=200
        )
        self.isolation_forest.fit(X_train)
    
    def build_lstm_model(self, input_shape):
        """LSTM for sequential pattern detection"""
        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=input_shape),
            LSTM(32),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['precision', 'recall'])
        return model
    
    def ensemble_predict(self, features):
        """Combine predictions from multiple models"""
        # Implementation for ensemble prediction
        pass

3. Kafka Producers and Consumers
python
# File: kafka_handlers.py
from kafka import KafkaProducer, KafkaConsumer
import json

class TransactionProducer:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
    
    def send_transaction(self, transaction_data):
        self.producer.send('transactions-raw', transaction_data)

class PredictionConsumer:
    def __init__(self):
        self.consumer = KafkaConsumer(
            'transactions-processed',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
    
    def process_predictions(self):
        for message in self.consumer:
            # Process transaction and generate prediction
            pass

Frontend Development (React + TypeScript)
1. Project Setup
bash
# Initialize React app with TypeScript
npx create-react-app fraud-detection-dashboard --template typescript
cd fraud-detection-dashboard

# Install required dependencies
npm install @mui/material @emotion/react @emotion/styled
npm install @mui/icons-material @mui/lab
npm install chart.js react-chartjs-2
npm install d3 @types/d3
npm install axios
npm install socket.io-client
npm install @reduxjs/toolkit react-redux

2. TypeScript Interfaces
typescript
// File: src/types/index.ts
export interface Transaction {
  id: number;
  transactionTime: string;
  timeFeature: number;
  v1: number; v2: number; v3: number; // ... v1-v28
  amount: number;
  actualClass?: number;
  createdAt: string;
}

export interface Prediction {
  id: number;
  transactionId: number;
  isolationForestScore: number;
  lstmPrediction: number;
  ensemblePrediction: number;
  finalPrediction: number;
  confidenceScore: number;
  modelVersion: string;
  predictionTime: string;
}

export interface FraudAlert {
  id: number;
  transactionId: number;
  alertLevel: 'HIGH' | 'MEDIUM' | 'LOW';
  alertReason: string;
  createdAt: string;
  acknowledged: boolean;
}

export interface DashboardMetrics {
  totalTransactions: number;
  fraudDetected: number;
  falsePositives: number;
  systemAccuracy: number;
  averageProcessingTime: number;
}

3. Dashboard Components
typescript
// File: src/components/Dashboard.tsx
import React, { useState, useEffect } from 'react';
import {
  Grid, Paper, Typography, Box, Alert,
  Table, TableBody, TableCell, TableContainer,
  TableHead, TableRow, Chip
} from '@mui/material';
import { Line, Doughnut, Bar } from 'react-chartjs-2';
import * as d3 from 'd3';

const Dashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<DashboardMetrics | null>(null);
  const [recentTransactions, setRecentTransactions] = useState<Transaction[]>([]);
  const [fraudAlerts, setFraudAlerts] = useState<FraudAlert[]>([]);

  // Real-time updates via WebSocket
  useEffect(() => {
    // WebSocket connection for real-time updates
  }, []);

  return (
    <Box sx={{ flexGrow: 1, p: 3 }}>
      <Typography variant="h4" gutterBottom>
        Real-Time Fraud Detection Dashboard
      </Typography>
      
      <Grid container spacing={3}>
        {/* Key Metrics Cards */}
        <Grid item xs={12} md={3}>
          <Paper sx={{ p: 2 }}>
            <Typography variant="h6">Total Transactions</Typography>
            <Typography variant="h4">{metrics?.totalTransactions || 0}</Typography>
          </Paper>
        </Grid>
        
        {/* Real-time Chart */}
        <Grid item xs={12} md={8}>
          <Paper sx={{ p: 2 }}>
            <Typography variant="h6">Transaction Flow</Typography>
            <Line data={transactionChartData} options={chartOptions} />
          </Paper>
        </Grid>
        
        {/* Fraud Alerts */}
        <Grid item xs={12} md={4}>
          <Paper sx={{ p: 2 }}>
            <Typography variant="h6">Active Fraud Alerts</Typography>
            {fraudAlerts.map(alert => (
              <Alert 
                key={alert.id} 
                severity={alert.alertLevel === 'HIGH' ? 'error' : 'warning'}
                sx={{ mb: 1 }}
              >
                Transaction #{alert.transactionId}: {alert.alertReason}
              </Alert>
            ))}
          </Paper>
        </Grid>
      </Grid>
    </Box>
  );
};

4. D3.js Visualizations
typescript
// File: src/components/D3Visualization.tsx
import React, { useRef, useEffect } from 'react';
import * as d3 from 'd3';

interface D3VisualizationProps {
  data: Transaction[];
}

const D3Visualization: React.FC<D3VisualizationProps> = ({ data }) => {
  const svgRef = useRef<SVGSVGElement>(null);

  useEffect(() => {
    if (!svgRef.current || !data.length) return;

    const svg = d3.select(svgRef.current);
    svg.selectAll("*").remove();

    // Create scatter plot for fraud detection
    const margin = { top: 20, right: 30, bottom: 40, left: 40 };
    const width = 800 - margin.left - margin.right;
    const height = 400 - margin.top - margin.bottom;

    const xScale = d3.scaleLinear()
      .domain(d3.extent(data, d => d.amount) as [number, number])
      .range([0, width]);

    const yScale = d3.scaleLinear()
      .domain(d3.extent(data, d => d.v1) as [number, number])
      .range([height, 0]);

    const g = svg.append("g")
      .attr("transform", `translate(${margin.left},${margin.top})`);

    // Add circles for each transaction
    g.selectAll("circle")
      .data(data)
      .enter().append("circle")
      .attr("cx", d => xScale(d.amount))
      .attr("cy", d => yScale(d.v1))
      .attr("r", 3)
      .style("fill", d => d.actualClass === 1 ? "red" : "blue")
      .style("opacity", 0.7);

  }, [data]);

  return <svg ref={svgRef} width={800} height={400}></svg>;
};

Docker Configuration
Docker Compose Setup
text
# File: docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: fraud_detection
      POSTGRES_USER: fraud_user
      POSTGRES_PASSWORD: fraud_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  fraud-detection-api:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - postgres
      - redis
      - kafka
    environment:
      DATABASE_URL: postgresql://fraud_user:fraud_password@postgres:5432/fraud_detection
      REDIS_URL: redis://redis:6379
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092

  fraud-detection-frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - fraud-detection-api

volumes:
  postgres_data:

Implementation Instructions
Phase 1: Environment Setup
Set up Docker containers for all services


Initialize PostgreSQL with schema


Configure Kafka topics


Set up Redis caching


Phase 2: Backend Development
Implement data preprocessing pipeline


Train ML models (Isolation Forest + LSTM)


Create ensemble prediction system


Implement Kafka producers/consumers


Build REST API endpoints


Phase 3: Frontend Development
Set up React TypeScript project


Implement Material-UI dashboard


Create Chart.js visualizations


Add D3.js interactive components


Implement real-time WebSocket connections


Phase 4: Integration & Testing
Connect all components


Test real-time data flow


Validate ML model performance


Implement monitoring and alerting


Key Features to Implement
Real-time Processing: Stream transactions through Kafka for immediate fraud detection


Model Ensemble: Combine Isolation Forest and LSTM for better accuracy


Class Imbalance Handling: Use SMOTE/ADASYN for training data augmentation


Interactive Dashboard: Real-time metrics, alerts, and visualizations


Performance Monitoring: Track model accuracy, precision, recall over time


Alert System: Immediate notifications for high-confidence fraud cases


This comprehensive system will handle your highly imbalanced fraud detection dataset with real-time processing capabilities and a modern, interactive dashboard for monitoring and analysis.
